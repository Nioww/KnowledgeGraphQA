{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvf = open(r'D:\\node.csv','w',newline='',encoding = 'utf-8')\n",
    "file = csv.writer(csvf)\n",
    "    #print(file[0])\n",
    "file.writerow((\"id:ID\",\"name\",\":LABEL\"))\n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = csv.writer(csvf)\n",
    "#w.writerow((\"id:ID\",\"name\",\":LABEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = r'D:\\pkubase-complete.txt'\n",
    "#file = open(path,'r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10):\n",
    "#    print(file[i])\n",
    "#file[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_node_file():\n",
    "    path = r'D:\\pkubase-complete.txt'\n",
    "    w_path = r'D:\\node.csv'\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        file = f.readlines()\n",
    "    print('数据库原始文件读取完成')\n",
    "    csvf = open(w_path,'w',newline='',encoding = 'utf-8')\n",
    "        \n",
    "\n",
    "    w = csv.writer(csvf)\n",
    "    w.writerow((\"id:ID\",\"name\",\":LABEL\"))\n",
    "    idx = 0\n",
    "    entity_dic = dict()\n",
    "    print(len(file))\n",
    "\n",
    "    for i in tqdm(range(int(len(file)/2))):#tqdm，进度条模块\n",
    "#         if '<类型>' in file[i]:\n",
    "#             continue\n",
    "#2019.8.30这次导入将实体的类型属性也导入数据库中\n",
    "            \n",
    "        entity1 = file[i].split('\\t')[0]\n",
    "        if entity1 not in entity_dic:\n",
    "            idx+=1\n",
    "            entity_dic[entity1]=idx\n",
    "            w.writerow((str(idx),entity1,'Entity'))\n",
    "        \n",
    "        entity2 = file[i].split('\\t')[2].rstrip(' .\\n')\n",
    "        if entity2 not in entity_dic:\n",
    "            idx+=1\n",
    "            entity_dic[entity2] = idx\n",
    "            w.writerow((str(idx),entity2,'Entity'))\n",
    "\n",
    "    print(len(entity_dic))\n",
    "    \n",
    "    csvf.close()\n",
    "    return entity_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库原始文件读取完成\n",
      "66499746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33249873/33249873 [01:23<00:00, 398931.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13407557\n"
     ]
    }
   ],
   "source": [
    "k = gen_node_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'ni shi sha bi\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ni shi sha bi\\t'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "path = r'C:\\Users\\HashiriNio\\Desktop\\ccks2019-ckbqa-4th-codes-master\\corpus\\task6ckbqa_train_2019.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(path,'r','utf-8') as f:\n",
    "    k = f.read().split('\\r\\n\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q1:莫妮卡·贝鲁奇的代表作？\\r\\nselect ?x where { <莫妮卡·贝鲁奇> <代表作品> ?x. }\\r\\n<西西里的美丽传说>',\n",
       " 'q2:《湖上草》是谁的诗？\\r\\nselect ?x where { ?x <主要作品> <湖上草>. }\\r\\n<柳如是_（明末\"秦淮八艳\"之一）>',\n",
       " 'q3:龙卷风的英文名是什么？\\r\\nselect ?x where { <龙卷风_（一种自然天气现象）> <外文名> ?x. }\\r\\n\"Tornado\"',\n",
       " 'q4:新加坡的水域率是多少？\\r\\nselect ?x where { <新加坡> <水域率> ?x. }\\r\\n\"1.444%\"',\n",
       " 'q5:商朝在哪场战役中走向覆灭？\\r\\nselect ?x where { <商朝> <灭亡> ?x. }\\r\\n<牧野之战>',\n",
       " 'q6:叔本华信仰什么宗教？\\r\\nselect ?y where { <亚瑟·叔本华> <信仰> ?y. }\\r\\n<佛教>',\n",
       " 'q7:大兴安岭的终点是哪里？\\r\\nselect ?x where { <大兴安岭> <终点> ?x. }\\r\\n<大窝口邨>',\n",
       " 'q8:演员梅艳芳有多高？\\r\\nselect ?y where { <梅艳芳> <身高> ?y. }\\r\\n\"168cm\"',\n",
       " 'q9:被誉为万岛之国的是哪个国家？\\r\\nselect ?x where { ?x <誉称> \"万岛之国\". }\\r\\n<挪威>',\n",
       " 'q10:北京奥运会的口号是什么？\\r\\nselect ?x where { <2008年北京奥运会 > <口号> ?x. }\\r\\n\"同一个世界，同一个梦想\"']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '莫妮卡·贝鲁奇的代表作？', 'answer': ['<西西里的美丽传说>'], 'entity': ['<莫妮卡·贝鲁奇>'], 'relation': ['<代表作品>']}\n",
      "{'question': '《湖上草》是谁的诗？', 'answer': ['<柳如是_（明末\"秦淮八艳\"之一）>'], 'entity': ['<湖上草>'], 'relation': ['<主要作品>']}\n",
      "{'question': '龙卷风的英文名是什么？', 'answer': ['\"Tornado\"'], 'entity': ['<龙卷风_（一种自然天气现象）>'], 'relation': ['<外文名>']}\n",
      "{'question': '新加坡的水域率是多少？', 'answer': ['\"1.444%\"'], 'entity': ['<新加坡>'], 'relation': ['<水域率>']}\n",
      "{'question': '商朝在哪场战役中走向覆灭？', 'answer': ['<牧野之战>'], 'entity': ['<商朝>'], 'relation': ['<灭亡>']}\n",
      "{'question': '叔本华信仰什么宗教？', 'answer': ['<佛教>'], 'entity': ['<亚瑟·叔本华>'], 'relation': ['<信仰>']}\n",
      "{'question': '大兴安岭的终点是哪里？', 'answer': ['<大窝口邨>'], 'entity': ['<大兴安岭>'], 'relation': ['<终点>']}\n",
      "{'question': '演员梅艳芳有多高？', 'answer': ['\"168cm\"'], 'entity': ['<梅艳芳>'], 'relation': ['<身高>']}\n",
      "{'question': '被誉为万岛之国的是哪个国家？', 'answer': ['<挪威>'], 'entity': ['\"万岛之国\"'], 'relation': ['<誉称>']}\n",
      "{'question': '北京奥运会的口号是什么？', 'answer': ['\"同一个世界，同一个梦想\"'], 'entity': ['<2008年北京奥运会 >'], 'relation': ['<口号>']}\n"
     ]
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i in range(10):\n",
    "    pair = {}\n",
    "    question = k[i].split('\\r\\n')[0].split(':')[1]\n",
    "    question = re.sub('我想知道','',question)\n",
    "    question = re.sub('你了解','',question)\n",
    "    question = re.sub('请问','',question)\n",
    "    answer = k[i].split('\\r\\n')[2].split('/t')\n",
    "    #answer = re.findall('<(.*?)>|\\\"(.*?)\\\"',answer)\n",
    "    sql = k[i].split('\\r\\n')[1]\n",
    "    sql = re.findall('{(.*?)}',sql)[0]\n",
    "    sql = re.findall('<.*?>|\\\".*?\\\"|\\?\\D',sql)\n",
    "    #print(question,sql,answer)\n",
    "    #pair[i] = (question,sql,answer)\n",
    "    entity = []\n",
    "    relation = []\n",
    "    for m in range(len(sql)):\n",
    "        if (m%3 == 1)and(sql[m] not in relation):\n",
    "            relation.append(sql[m])\n",
    "        else:\n",
    "            if (sql[m][0] != '?')and(sql[m] not in entity):\n",
    "                entity.append(sql[m])\n",
    "    #print(entity,relation)\n",
    "    pair['question'] = question\n",
    "    pair['answer'] = answer\n",
    "    pair['entity'] = entity\n",
    "    pair['relation'] = relation\n",
    "    print(pair)\n",
    "    corpus[i] = pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'question': '莫妮卡·贝鲁奇的代表作？', 'answer': ['<西西里的美丽传说>'], 'entity': ['<莫妮卡·贝鲁奇>'], 'relation': ['<代表作品>']}, 1: {'question': '《湖上草》是谁的诗？', 'answer': ['<柳如是_（明末\"秦淮八艳\"之一）>'], 'entity': ['<湖上草>'], 'relation': ['<主要作品>']}, 2: {'question': '龙卷风的英文名是什么？', 'answer': ['\"Tornado\"'], 'entity': ['<龙卷风_（一种自然天气现象）>'], 'relation': ['<外文名>']}, 3: {'question': '新加坡的水域率是多少？', 'answer': ['\"1.444%\"'], 'entity': ['<新加坡>'], 'relation': ['<水域率>']}, 4: {'question': '商朝在哪场战役中走向覆灭？', 'answer': ['<牧野之战>'], 'entity': ['<商朝>'], 'relation': ['<灭亡>']}, 5: {'question': '叔本华信仰什么宗教？', 'answer': ['<佛教>'], 'entity': ['<亚瑟·叔本华>'], 'relation': ['<信仰>']}, 6: {'question': '大兴安岭的终点是哪里？', 'answer': ['<大窝口邨>'], 'entity': ['<大兴安岭>'], 'relation': ['<终点>']}, 7: {'question': '演员梅艳芳有多高？', 'answer': ['\"168cm\"'], 'entity': ['<梅艳芳>'], 'relation': ['<身高>']}, 8: {'question': '被誉为万岛之国的是哪个国家？', 'answer': ['<挪威>'], 'entity': ['\"万岛之国\"'], 'relation': ['<誉称>']}, 9: {'question': '北京奥运会的口号是什么？', 'answer': ['\"同一个世界，同一个梦想\"'], 'entity': ['<2008年北京奥运会 >'], 'relation': ['<口号>']}}\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['?x','\"Tornado\"']\n",
    "y = re.findall('\\?\\D|\\\".+?\\\"',x[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if x!='?':\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import Tokenizer\n",
    "import codecs\n",
    "dict_path = r'C:\\Users\\HashiriNio\\Desktop\\final_design\\Final_one\\bert\\vocab.txt'\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(dict_path,'r','utf-8') as f:\n",
    "    for i in f:\n",
    "        token = i.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "        \n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 5811, 1984, 1305, 185, 6564, 7826, 1936, 4638, 807, 6134, 868, 117, 784, 720, 7787, 8043, 102, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) ([101, 5811, 1984, 1305, 185, 6564, 7826, 1936, 4638, 807, 6134, 868, 135, 102, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "x = '莫妮卡·贝鲁奇的代表作,什么鬼？'\n",
    "z = '莫妮卡·贝鲁奇的代表作>'\n",
    "y = tokenizer.encode(first = x,max_len = 20)\n",
    "k = tokenizer.encode(first = z,max_len = 20)\n",
    "print(y,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25d5291d1352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'modeling'"
     ]
    }
   ],
   "source": [
    "import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Python 3.7's help utility!\n",
      "\n",
      "If this is your first time using Python, you should definitely check out\n",
      "the tutorial on the Internet at https://docs.python.org/3.7/tutorial/.\n",
      "\n",
      "Enter the name of any module, keyword, or topic to get help on writing\n",
      "Python programs and using Python modules.  To quit this help utility and\n",
      "return to the interpreter, just type \"quit\".\n",
      "\n",
      "To get a list of available modules, keywords, symbols, or topics, type\n",
      "\"modules\", \"keywords\", \"symbols\", or \"topics\".  Each module also comes\n",
      "with a one-line summary of what it does; to list the modules whose name\n",
      "or summary contain a given string such as \"spam\", type \"modules spam\".\n",
      "\n",
      "help> keras.layes\n",
      "No Python documentation found for 'keras.layes'.\n",
      "Use help() to get the interactive help utility.\n",
      "Use help(str) for help on the str class.\n",
      "\n",
      "help> keras.layers\n",
      "Help on package keras.layers in keras:\n",
      "\n",
      "NAME\n",
      "    keras.layers\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    advanced_activations\n",
      "    convolutional\n",
      "    convolutional_recurrent\n",
      "    core\n",
      "    cudnn_recurrent\n",
      "    embeddings\n",
      "    experimental (package)\n",
      "    local\n",
      "    merge\n",
      "    noise\n",
      "    normalization\n",
      "    pooling\n",
      "    recurrent\n",
      "    wrappers\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\__init__.py\n",
      "\n",
      "\n",
      "help> quit\n",
      "\n",
      "You are now leaving help and returning to the Python interpreter.\n",
      "If you want to ask for help on a particular object directly from the\n",
      "interpreter, you can type \"help(object)\".  Executing \"help('string')\"\n",
      "has the same effect as typing a particular string at the help> prompt.\n"
     ]
    }
   ],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x1 = np.array([1,2,3,4,5,6])\n",
    "x2 = np.array([5,1,3,4,8,6])\n",
    "b = tf.abs(tf.math.subtract(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.dot(x1,x2)\n",
    "a = np.sqrt(np.dot(x1,x1))\n",
    "c = np.sqrt(np.dot(x2,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921328669759435"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/(a*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "k = tf.math.reduce_mean([2,11])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1,2,3],\n",
    "      [1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = tf.math.reduce_mean(x1,0)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\data_v2.json'\n",
    "f = open(path,encoding = 'utf-8')\n",
    "res = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dic = []\n",
    "entity_dic = []\n",
    "for i in res:\n",
    "    if i['type'] not in type_dic:\n",
    "        type_dic.append(i['type'])\n",
    "    if i['name'] not in entity_dic:\n",
    "        entity_dic.append(i['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity count: 127\n",
      "type count: 11\n"
     ]
    }
   ],
   "source": [
    "print('entity count:',len(entity_dic))\n",
    "print('type count:',len(type_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
