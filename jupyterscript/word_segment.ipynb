{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9ba4f0f90661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#import time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_bert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_custom_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_bert\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_bert\\bert.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_pos_embd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPositionEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_layer_normalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_transformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_encoders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_transformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_custom_objects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mget_encoder_custom_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_pos_embd\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpos_embd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPositionEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtrig_pos_embd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrigPosEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_pos_embd\\pos_embd.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPositionEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_pos_embd\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import codecs as cs\n",
    "import pickle\n",
    "#import time\n",
    "import numpy as np\n",
    "from keras_bert import Tokenizer,get_custom_objects\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = pickle.load(open(r'C:\\Users\\HashiriNio\\Desktop\\final_design\\Final_one\\data\\corpus_test.pkl','rb'))\n",
    "test_questions = [test_corpus[i]['question'] for i in range(len(test_corpus))]\n",
    "test_entitys = [test_corpus[i]['gold_entitys'] for i in range(len(test_corpus))]\n",
    "test_entitys = [[entity[1:-1].split('_')[0] for entity in line]for line in test_entitys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityLinker():\n",
    "    def __init__(self):\n",
    "        self.seq_max_len = 20\n",
    "        #加载bert字典\n",
    "        dict_path = r'C:\\Users\\HashiriNio\\Desktop\\final_design\\Final_one\\bert\\vocab.txt'\n",
    "        token_dict = {}\n",
    "        with cs.open(dict_path, 'r', 'utf8') as reader:\n",
    "            for line in reader:\n",
    "                token = line.strip()\n",
    "                token_dict[token] = len(token_dict)\n",
    "        self.tokenizer = Tokenizer(token_dict)\n",
    "        \n",
    "        #加载NER模型\n",
    "        model_path = r'C:\\Users\\HashiriNio\\Desktop\\final_design\\Final_one\\data\\model\\model_ner_general.h5'\n",
    "        custom_objects = get_custom_objects()\n",
    "        self.ner_model = load_model(model_path,custom_objects =custom_objects)\n",
    "        \n",
    "        #加载实体词典\n",
    "        mention_dic_path = r'C:\\Users\\HashiriNio\\Desktop\\final_design\\Final_one\\data\\mention_dict.txt'\n",
    "        mention_dict = {}\n",
    "        with open(mention_dic_path,'r',encoding = 'utf-8') as f:\n",
    "            for i in f:\n",
    "                if i.strip():\n",
    "                    mention_dict[i.strip()] = 1\n",
    "        self.mention_dict = mention_dict\n",
    "        #jieba.load_userdict('mention_dic_path')\n",
    "        print('clear')\n",
    "    \n",
    "    def restore_entity_from_corpus(self,predicty,question):\n",
    "        def restore_entity_from_labels(labels,question):\n",
    "            entitys = []\n",
    "            str = ''\n",
    "            labels = labels[1:-1]\n",
    "            #print(labels,question)\n",
    "            for i in range(min(len(labels),len(question))):\n",
    "                if labels[i]==1:\n",
    "                    str += question[i]\n",
    "                else:\n",
    "                    if len(str):\n",
    "                        entitys.append(str)\n",
    "                        str = ''\n",
    "            if len(str):\n",
    "                entitys.append(str) \n",
    "            return entitys\n",
    "        all_entitys = []\n",
    "        for i in range(len(predicty)):\n",
    "            all_entitys.append(restore_entity_from_labels(predicty[i],question[i]))\n",
    "            #print(predicty[i])\n",
    "        return all_entitys\n",
    "    \n",
    "    def mention_get(self,question):\n",
    "        #ner\n",
    "        q = question\n",
    "        mention_ner = []\n",
    "        i1,i2 = self.tokenizer.encode(first = q,max_len = self.seq_max_len)\n",
    "        i1 = np.array(i1)\n",
    "        i2 = np.array(i2)        \n",
    "        pre = self.ner_model.predict([np.array([i1,i2]),np.array([i2,i2])])\n",
    "        pre = [1 if i >0.5 else 0 for i in pre[0]]\n",
    "        #print(pre)\n",
    "        predicty = self.restore_entity_from_corpus([pre],[q])\n",
    "        print(predicty)\n",
    "        mention_ner = predicty[0]\n",
    "        #for i in self.mention_dict.keys():\n",
    "        #    for j in predicty[0]:\n",
    "        #       if j in i:\n",
    "        #            mention_ner.append(i)\n",
    "        \n",
    "        #分词\n",
    "        mentions = []\n",
    "        tokens = jieba.lcut(question)\n",
    "        for t in tokens:\n",
    "            if t in self.mention_dict.keys():\n",
    "                mentions.append(t)\n",
    "        \n",
    "        mentionall = mentions + mention_ner\n",
    "        return mentionall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HASHIR~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.673 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['计算机网络', '是', '什么']\n"
     ]
    }
   ],
   "source": [
    "question = '计算机网络是什么'\n",
    "tokens = jieba.lcut(question)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "me = EntityLinker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HASHIR~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['伯瓦尔·弗塔根', '魔兽世界']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.738 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['伯',\n",
       " '瓦尔',\n",
       " '·',\n",
       " '在',\n",
       " '游戏',\n",
       " '《',\n",
       " '魔兽',\n",
       " '世界',\n",
       " '》',\n",
       " '中',\n",
       " '的',\n",
       " '身份',\n",
       " '是',\n",
       " '？',\n",
       " '伯瓦尔·弗塔根',\n",
       " '魔兽世界']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '伯瓦尔·弗塔根在游戏《魔兽世界》中的身份是？'\n",
    "entity = me.mention_get(q)\n",
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['伯',\n",
       " '瓦尔',\n",
       " '·',\n",
       " '在',\n",
       " '游戏',\n",
       " '《',\n",
       " '魔兽',\n",
       " '世界',\n",
       " '》',\n",
       " '中',\n",
       " '的',\n",
       " '身份',\n",
       " '是',\n",
       " '？',\n",
       " '傻逼']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '伯瓦尔·弗塔根在游戏《魔兽世界》中的身份是？'\n",
    "#tokens = jieba.lcut(question)\n",
    "tokens = ['伯', '瓦尔', '·', '在', '游戏', '《', '魔兽', '世界', '》', '中', '的', '身份', '是', '？','傻逼']\n",
    "mentions=[]\n",
    "for t in tokens:\n",
    "    for m in me.mention_dict.keys():\n",
    "        if m == t:\n",
    "            mentions.append(t)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['锁清秋',\n",
       " '张志华',\n",
       " '熊彦',\n",
       " '手绘效果图表现技法',\n",
       " '陈防',\n",
       " '澡塘自然村',\n",
       " '莲花池',\n",
       " 'hold住爱',\n",
       " 'JOKER',\n",
       " '白黄']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(me.mention_dict.keys())[111500:111510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['傻逼']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention = {'傻逼':1,'魔兽世界':2}\n",
    "tokens = ['伯', '瓦尔', '·', '在', '游戏', '《', '魔兽', '世界', '》', '中', '的', '身份', '是', '？','傻逼']\n",
    "mentions=[]\n",
    "for t in tokens:\n",
    "    for m in mention.keys():\n",
    "        if m == t:\n",
    "            mentions.append(t)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
